# ğŸš¨ CRITICAL: GPU RAM Prevention in Tests

## Problem Solved

**BEFORE:** Tests were using different LLM models than the main config, causing:
- âŒ GPU RAM exhaustion 
- âŒ CPU fallback (extremely slow)
- âŒ Test timeouts and failures
- âŒ Inconsistent test performance

**AFTER:** All tests now use the **EXACT SAME MODEL** as main config:
- âœ… `qwen3:14b` (matches `config/sync2nas_config.ini`)
- âœ… No GPU RAM issues
- âœ… Fast test execution
- âœ… Consistent performance

## What Was Fixed

### ğŸ”§ **106 Model References Fixed Across 20 Files**

The following incorrect models were replaced with `qwen3:14b`:
- `ollama3.2` (invalid model name)
- `llama3.2` (different model)
- `gpt-4` (different service)
- `claude-3-haiku` (different service)

### ğŸ“ **Files Fixed:**
- `tests/cli/test_*.py` - CLI test files
- `tests/services/test_*.py` - Service test files  
- `tests/utils/test_*.py` - Utility test files
- `tests/integration/test_*.py` - Integration test files
- `tests/conftest.py` - Global test configuration

## Mock Service Factory Enhancements

### ğŸ›¡ï¸ **Critical Safety Features Added:**

1. **`STANDARD_TEST_MODEL = "qwen3:14b"`**
   - Centralized constant for the correct model
   - Matches main config exactly

2. **`get_standard_test_config()`**
   - Returns standardized config with correct model
   - Uses in-memory SQLite for speed

3. **`ensure_test_model_consistency()`**
   - Forces any config to use the correct model
   - Handles both lowercase and uppercase sections

4. **`patch_llm_service_creation()`**
   - Context manager to mock LLM services
   - Prevents real model loading in tests

5. **`fix_test_model_usage()`**
   - Automated function to fix incorrect models
   - Scans and updates all test files

## Usage Guidelines

### âœ… **DO THIS in Tests:**

```python
# Use the mock service factory
from tests.utils.mock_service_factory import MockServiceFactory

# Get standardized config
config = MockServiceFactory.get_standard_test_config()

# Create mock services (no real LLM loading)
llm_service = MockServiceFactory.create_mock_llm_service(config)

# Use the LLM patch fixture
def test_something(mock_llm_service_patch):
    # LLM services are automatically mocked
    pass
```

### âŒ **DON'T DO THIS:**

```python
# DON'T use different models
config = {"ollama": {"model": "llama3.2"}}  # WRONG!
config = {"ollama": {"model": "gpt-4"}}     # WRONG!

# DON'T create real LLM services in tests
from services.llm_factory import create_llm_service
service = create_llm_service(config)  # WILL LOAD REAL MODEL!
```

## Verification

### ğŸ§ª **How to Check Model Usage:**

```bash
# Search for incorrect models in tests
grep -r "llama3.2\|ollama3.2\|gpt-4" tests/

# Should return NO results after fix
```

### ğŸ” **Run the Model Fixer:**

```python
# Fix any new incorrect model usage
python -c "
import sys; sys.path.insert(0, '.'); 
from tests.utils.mock_service_factory import fix_test_model_usage; 
fix_test_model_usage()
"
```

## Performance Impact

### âš¡ **Before vs After:**

| Metric | Before | After |
|--------|--------|-------|
| GPU RAM Usage | ğŸ”´ High (multiple models) | ğŸŸ¢ Low (single model) |
| Test Speed | ğŸ”´ Slow (CPU fallback) | ğŸŸ¢ Fast (GPU/mocks) |
| Consistency | ğŸ”´ Variable | ğŸŸ¢ Consistent |
| Reliability | ğŸ”´ Timeouts/failures | ğŸŸ¢ Reliable |

## Maintenance

### ğŸ”„ **When Adding New Tests:**

1. **Always use `MockServiceFactory.get_standard_test_config()`**
2. **Never hardcode model names in tests**
3. **Use the `mock_llm_service_patch` fixture**
4. **Run the model fixer if you see GPU issues**

### ğŸš¨ **Warning Signs of Model Issues:**

- Tests suddenly become very slow
- GPU memory errors in logs
- Tests timing out
- Inconsistent test performance

### ğŸ› ï¸ **Quick Fix Command:**

```bash
# If you suspect model issues, run this:
python -c "import sys; sys.path.insert(0, '.'); from tests.utils.mock_service_factory import fix_test_model_usage; fix_test_model_usage()"
```

## Summary

This fix ensures that **ALL TESTS USE THE SAME MODEL** as the main configuration (`qwen3:14b`), preventing GPU RAM exhaustion and ensuring fast, consistent test execution.

**Key Rule:** Tests should NEVER load different LLM models than what's already in GPU memory.
